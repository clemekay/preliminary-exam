
\chapter{Introduction and Background} \label{ch-introduction}
\section{A brief overview}\label{sec:overview}
As computational modeling becomes more important to the scientific community, so does the necessity of quantifying and analyzing model reliability, accuracy, and robustness. As such, uncertainty quantification (UQ) and global sensitivity analysis (GSA) are important parts of the modeling workflow. Uncertainty quantification has been assigned a number of overlapping definitions - we refer to the mathematical characterization of how sources of input uncertainty affect model output. Global sensitivity analysis can be viewed as a broadening of uncertainty quantification: GSA aims to apportion, or divide and allocate, uncertainty in model output to different sources of uncertainty in the model input~\cite{saltelli2004}. Another way to phrase this is that GSA aims to understand the relative importance of each of the uncertain inputs, as well as their interactions with one another, to the behavior of model output. To apportion model output uncertainty, one must first quantify the uncertainty effects of each input. In this way, UQ and GSA go hand-in-hand, with UQ typically acting as a step in GSA. 
There are a wide range of UQ and GSA methodologies which are applicable in different modeling scenarios. Typically, these methodologies assume that the computational model itself is deterministic, \textit{i.e.} that given the same input, the model will produce the same output. This way, when analyzing the effects of uncertain input parameters, UQ and GSA methodologies can assume that any output variability is caused by input variability. If the model incorporates some non-deterministic, \textit{i.e.} stochastic, behavior, that isn't necessarily the case. A stochastic model that nuclear engineers are likely familiar with is a Monte Carlo radiation transport solver, in which average particle behavior is modeled by sampling probability distributions that describe physical phenomena. In that case, the variance of the model's output is a function (amongst other things) of the number of particle histories used in the simulation. In the case of such a model, the typical UQ workflow which assumes that the output uncertainty can be analyzed as solely the function of uncertain input parameters is not applicable. This of course propagates through to GSA, absorbing the effects of the solver's stochastic behavior into the output uncertainty, apportioning it to the various UQ inputs, improperly analyzing the effects of the uncertain input sources. 

Stochastic solvers are widely used and important for many applications in which physical systems are difficult or outright impossible to describe with deterministic models, and UQ and GSA are too important to forego simply because stochastic solvers make the process less straightforward. A brute-force method to address this complication is to over-resolve the stochastic solver, \eg increase the number of particles in the simulation to drive down the variance of the output, rendering it negligible compared to the effects of the uncertain input. Resolving stochastic models to this extent is already computationally expensive, and folding that into the UQ and GSA workflow which requires repeated evaluation of numerical codes increases the computational expense to the point of intractability. As problems become more complex, UQ and GSA become more important, while more and more histories are needed to use the brute-force method to over-resolve the radiation transport problem such that the solver variance can be assumed negligible, and the problem compounds. 

The purpose of this proposal is to quantify and mitigate the effects of using a stochastic solver as part of UQ and GSA. This work continues ongoing efforts in this space, and the expected outcome is an in-depth discussion of how stochastic solvers can be incorporated into these methods, their performance, and their trade-offs over a collection of realistic scenarios.
While these methods are developed with nuclear engineering applications in mind, the work and its results are applicable to a wide range of spaces which use stochastic modeling such as fluid flow, plasmas, etc.
The remainder of this section provides background on uncertainty quantification, global sensitivity analysis, and Monte Carlo radiation transport. Section~\ref{ch-objectives} is a detailed outline of the research objectives of this proposal. Section~\ref{ch-method} provides the proposed methodology for accomplishing these objectives, as well as supporting theory. Section~\ref{ch-results} contains results for work performed thus far and Section~\ref{ch-schedule} includes a timeline for the remaining work that will form the basis of the PhD dissertation. 


\section{Background: Uncertainty quantification}\label{sec:background-uq}
Scientific progress involves comparing predictions from theory with experimental evidence, with updates to these two pillars happening in tandem to inform one another. Scientists observe and collect data about the physical world, use real-world data to validate or refine models and theories, use those models and theories to predict what other physical phenomena might be observed, compare the models to the data by observing the physical world, and so on and so forth. In \cite{ghanem-uq-handbook}, the authors describe uncertainty quantification as "the rational process by which proximity between predictions and observations is characterized." The process combines applied mathematics, computational science, statistics, and practical knowledge to characterize how well a computational model describes an existing system, or the reliability of the model in predicting new systems.

% Computational models are sometimes described as ``black-boxes,'' which conjure up a useful image for understanding what computational models are. You provide the black-box with some information about a real-world system, it performs some simulation, and outputs some system behavior. 

Computational models are widely useful across any discipline which hopes to take some real-world process -- how the economy might perform under various circumstances, how radiation might move through a system, how disease might be transmitted through a population -- and understand it without having to perform real-world experiments, or when real-world experimentation is not possible. Ideally, a computational model can be built and perfected with existing experimental data, and then used to predict how the system will behave when new conditions are introduced. A model's accuracy can be tested by comparing its prediction to how the system performs in the physical world.

There are a number of types of computational models. One such example is a polynomial whose coefficients are computed using existing data -- by plugging the existing data back into the model, one could ensure that they receive the correct known response back. Assuming the model were perfect, one could then plug new inputs into the model how the system would behave. In the nuclear engineering field, we may be more familiar with thinking of computational models as geometric descriptions of systems which incorporate material data to predict how radiation will be transported throughout the system. In this case as well, verification could be performed by comparing with existing analytic benchmarks and experimental data, and once the model has been verified, the model's behavior can be tested under new conditions.

The uncertainty which UQ hopes to quantify can come from a number of sources. Assuming the model is constructed and verified with perfectly accurate data, approximations used to build the model will introduce some discrepancy between the model output and the true real-world behavior. Unfortunately, we can assume that data is not perfectly accurate; even at the limit of perfectly collected data, there will be some unpredictable and unavoidable imperfection. The different types of uncertainty are sometimes broadly categorized as \textit{epistemic} and \textit{aleatoric}~\cite{matthies}. Epistemic uncertainty, \textit{i.e.} systematic uncertainty, arises from the imperfect approximation of system conditions. Approximations may be used for any number of reasons, including assumption that the effects of approximation are negligible, hopes of avoiding computational complexity and expense, or simply the limit of current modeling capabilities. Aleatoric uncertainty, \textit{i.e.} stochastic uncertainty, describes the impossibility of perfect data. There will be some random imperfections in the machinery, the experiment tools, the conditions of the experiment, etc. which will introduce some uncertainty to the problem. Mathematically, aleatoric uncertainty describes the case in which a system is described by a probability distribution, but the value of a drawn sample cannot be known a priori. Epistemic uncertainty describes the case in which the system can not be well-described by a probability distribution, or it's not known how well the distribution describes relevant behavior \cite{smithUQ}. 
% Modern discussion of uncertainty quantification has included questioning whether it is useful or even possible to distinguish between the two types of uncertainty. Any model will use some kind of approximation, and will be built using data that has some aleatoric uncertainty \cite. 
The methods used for UQ depend on the goal of the UQ. In inverse problems, experimental data are used to simultaneously characterize and solve for unknown model parameters~\cite{wildey-presentation}; for example, when new data are used to update and correct models ~\cite{ghanem-uq-handbook, inverse}.
% or when the solution of the system depends on itself~\cite{ghanem-uq-handbook, inverse, pde}. 
The Bayesian approach is a common UQ method for solving inverse problems~\cite{wildey-presentation, ghanem-uq-handbook}. In forward problems, sources of uncertainty are propagated through a model, often with the aim of computing low order moments like mean and variance of the model output~\cite{ghanem-uq-handbook}, with the intention of evaluating how the system will respond to uncertainty sources and how sensitive the model response is to various sources of uncertainty~\cite{saltelliGSA}.

The focus of this dissertation proposal is on forward-propagation of aleatoric uncertainty, particularly using Monte Carlo sampling methods. In Monte Carlo UQ (MCUQ), input parameters are randomly sampled from known probability distributions and used to perform a simulation. This is repeated a number of times such that useful information can be gleaned from the series of outputs: sampling approximations for central moments like average response and variance, a sample-based probability distribution function and cumulative density function of system responses, and the envelope of possible system responses, for example~\cite{wildey-presentation}. Unlike polynomial model construction, the cost of MCUQ is independent of problem dimensionality, and converges reliably to a mean system response, albeit slowly, a s $\mathcal{O}(N^{-1/2}$ for $N$ independent simulations~\cite{mcnp}. In multi-level and multi-fidelity methods, higher- and lower-fidelity models can be leveraged to allocate the number of samples used and take advantage of this convergence~\cite{wildey-presentation}. Workflows for MCUQ are well-established and continue to be optimized for various application spaces~\cite{zhang-survey}. For complex systems, deterministic models are not always well-suited to capture system behavior. One such case is in the use of Monte Carlo radiation transport (MCRT) solvers, wherein the behavior of a fixed number of particles is simulated, and the model response is an average of the particle behavior. As with MCUQ, the model output of MCRT solvers converges $\mathcal{O}(N_p^{-1/2}$ for $N_p$ independent particle histories \cite{mcnp}. If traditional MCUQ methods are applied to stochastic computational models, the subsequent UQ statistical analysis can be considered ``polluted'' by the variability introduced by the solver itself. When integrating MCRT codes into a UQ workflow, a brute-force method to ``de-pollute'' the UQ statistics is to drive down the MCRT solver variability by increasing the number of particle histories \cite{MCMC-paper}. With this method, the variability contribution from the MCRT solver is assumed to be negligible compared to the variability one hopes to analyze, and the UQ workflow proceeds. However, as mentioned, the variability of the MCRT response is proportional to the inverse of the squared number of particle histories used, likely increasing the computational expense to the point of intractability when combined with the repeated runs necessary for MCUQ. 

A recently derived alternative approach is \textit{variance deconvolution} \cite{Clements2021, ClementsANS2022}, a framework which quantifies the variance contribution from a stochastic solver and effectively removes it from the total polluted variance, therefore calculating the desired UQ statistics. This is far more cost effective than the brute-force approach, and uses an unbiased estimator for the variance introduced by the solver and for the UQ variance (or parametric variance). We apply this variance deconvolution UQ workflow here to MCRT problems, but as we will show, the method is not specific to radiation transport and is widely applicable with solvers that use a Monte Carlo method. 

\section{Background: Global sensitivity analysis}\label{sec:background-gsa}
Global sensitivity analysis is the study of the apportionment (division, or allocation) of model output uncertainty to various sources of uncertainty in the model input~\cite{saltelli2004}. 
% Another way of phrasing this is understanding the relative importance of each of the uncertain inputs and their interactions with one another on the behavior of model output. To apportion model output uncertainty to various inputs, one must first quantify the uncertainty effects of each of those inputs. In this way, UQ and GSA go hand-in-hand, with UQ typically acting as a step in the GSA workflow. 
As with uncertainty quantification, global sensitivity analysis is extremely important in understanding model quality and reliability, and should be treated as an important step in the modeling and simulation workflow across all disciplines. There are a number of rationales for incorporating GSA into the computational modeling workflow~\cite{saltelli2005}:
\begin{itemize}
    \item Model reliability: Is the model overly-dependent on data which is very uncertain? Does model uncertainty seem independent of input uncertainty?
    \item Model improvement: Can the model be simplified by simplifying treatment of low-impact parameters? Are there any regions that could benefit from greater study or resolution?
    \item Data prioritization: Factor importance to model output variability correlates with the importance of factor analysis. Would improving the uncertainty of one factor greatly improve output stability? Should the interactions between two inputs be further studied?
    \item Protection against erroneous assumptions: Studying sensitivity globally prevents drawing improper conclusions based on specific sampled input parameters.
\end{itemize}
% An additional rationale for GSA which is less-easily bulleted is protection against assumptions which are erroneous but unidentifiable with a finite set of studied parameters. If MCUQ is only performed with a sampled set of input parameters, only the effects of those parameters can be studied. GSA provides a straightforward and efficient way to know whether the parameters' interactions with one another is also of large relative importance. Without this, it could be the case that the specifically sampled input parameters are interacting in a way that is favorable to the output variability, but which is not generally the case, leading to incorrect assumptions about system behavior. 
% What is actually studied in GSA? 
One way GSA is performed is by computing an importance index for each of the uncertain input parameters~\cite{sobol}. Given some generic model $Y=f(X_1,X_2,\cdots,X_k)$ with $k$ uncertain input parameters $X$, consider setting one of the factors $X_i$ equal to a particular constant $x_i$. If we take the variance of Y over all of the non-$i$ factors $X_{\sim i}$. With $X_i$ frozen, $\mathbb{V}ar_{X_{\sim i}}\left[ Y \g X_i = x_i \right]$ is the variance of $Y$ dependent on all factors \textit{but} $X_i$. This is known as a conditional variance, because it is conditional on $X_i$ being fixed to $x_i$. To ensure that this conditional variance is not dependent on a particular value of $x_i$, we can compute the average $\mathbb{V}ar_{X_{\sim i}}\left[ Y \g X_i = x_i \right]$ over all possible values of $x_i$, $\mathbb{E}_{X_i}\big[ \mathbb{V}ar_{X_{\sim i}}\left[ Y | X_i\right]\big]$. Using the law of total variance \cite{saltelliGSA},
\begin{equation}
    \mathbb{E}_{X_i}\big[ \mathbb{V}ar_{X_{\sim i}}\left[ Y | X_i \right] \big] + 
    \mathbb{V}ar_{X_i}\big[  \mathbb{E}_{X_{\sim i}}\left[ Y | X_i \right] \big] = \mathbb{V}ar_{X_i}\left[Y\right] .
\end{equation}
One can imagine that if $\mathbb{E}_{X_i}\big[ \mathbb{V}ar_{X_{\sim i}}\left[ Y | X_i \right] \big]$ is small, this indicates that all factors which are \textit{not} $X_i$ have a low impact on the output variance; a large $\mathbb{V}ar_{X_i}\big[  \mathbb{E}_{X_{\sim i}}\left[ Y | X_i \right] \big]$ indicates the same. The conditional variance $\mathbb{V}ar_{X_i}\big[  \mathbb{E}_{X_{\sim i}}\left[ Y | X_i \right] \big]$ is also known as the first-order effect of $X_i$ on $Y$. The ratio of $X_i$'s first-order effect on $Y$ to the total variance of $Y$,
\begin{equation}\label{eq:first-order-si}
    S_i = \frac{\mathbb{V}ar_{X_i}\big[  \mathbb{E}_{X_{\sim i}}\left[ Y | X_i \right] \big]}{\mathbb{V}ar\left[Y\right]} ,
\end{equation}
is known as the first-order sensitivity index (SI) of $X_i$ on $Y$, a numerical measure of the importance of $X_i$ on the variance of $Y$~\cite{saltelliGSA}.  $S_i$ can range between 0 and 1, where the importance of the variable increases as its $S_i$ approaches 1. (Because these sensitivity indices were first introduced by Ilya Sobol'~\cite{sobol}, they are also sometimes referred to as Sobol' indices.)
We have also referenced the fact that two input parameters may interact in a way that also affects the variance. In terms of sensitivity indices, two factors are said to have an interaction effect when their combined effect on $Y$ can not be fully described by the sum of their individual first order effects \cite{saltelliGSA}. The interaction effect of the pair $(X_i,X_j)$ is measured by removing their individual first-order effects from their combined effect. Introducing some shorthand notation,
\begin{equation}\label{eq:interaction-effect}
    \mathbb{V}_i = \mathbb{V}ar_{X_i}\big[  \mathbb{E}_{X_{\sim i}}\left[ Y | X_i \right] \big],
\end{equation}
\begin{equation*}
    \mathbb{V}_{ij} = \mathbb{V}ar\big[ \mathbb{E}\left[ Y \g X_i,X_j\right] \big] - \mathbb{V}_i - \mathbb{V}_j.
\end{equation*}
The term $\mathbb{V}_{ij}$ is the second-order effect, and this methodology can be extended for higher-order effects. From this, we can compute the \textit{total effect} of $X_i$, which accounts for the individual effect of $X_i$ plus all of its interaction effects. For a model with three uncertain input factors, $Y = f(X_1,X_2,X_3)$, the total effect of $X_1$ is the sum of all terms which include $X_1$:
\begin{equation}\label{eq:total-effect-si}
    S_{T1} = S_1 + S_{12} + S_{13} + S_{123} .
\end{equation}

Though not all GSA methods are variance-based, variance- and variance-decomposition-based techniques are of particular interest in our application space. This is not only to take advantage of implementing the variance deconvolution technique introduced in Section~\ref{sec:background-uq}, but also because of their applicability towards some important sensitivity tests. As outlined earlier, there are a number of questions one might hope to answer using these sensitivity indices; analogously, there are a number of sensitivity tests in GSA whose usefulness depends on the problem definition and type of uncertainty. Most relevant to this work are~\cite{saltelliGSA}:
\begin{itemize}
    \item Factor Prioritization (FP). Used to identify the input or group of inputs whose variability accounts for most of the output variability. Once identified, focus can be shifted towards reducing the variability of these parameters.
    \item Factor Fixing (FF). Used to identify the input or group of inputs whose variability makes little to no contribution to the output variability. Once identified, these parameters can essentially be set at some arbitrary value within their probability distribution, because varying them does not largely affect the output.
    \item Variance Cutting (VC). Used to identify the smallest set of factors one could act upon in order to reduce the output variance below a given threshold. This ensures the most effective optimization for a given output uncertainty goal.
\end{itemize}

The Saltelli method~\cite{saltelliGSA} for computing sensitivity indices using Monte-Carlo based sampling is described below for a model with $k$ uncertain inputs:
\begin{enumerate}
    \item Generate two $(N,k)$ matrices of random numbers, $A$ and $B$. The base sample $N$ is a number of independent re-samplings of input parameters that can range from a few hundreds to a few thousands. 
    \item Form the matrix $C_i$ by replacing the $i^{th}$ column of $B$ with the $i^{th}$ column of $A$.
    \item Compute the model output as a function of input matrices $A$, $B$, and $C_i$ to obtain $N\times 1$ vectors of model output $y_A=f(A)$, $y_B=f(B)$, and $y_{Ci}=f(C_i)$. 
    \item For all $k$ columns of $A$, construct $C_i$ and compute $y_{Ci}$.
\end{enumerate}
First-order sensitivity indices can be estimated:
\begin{equation}\label{eq:saltelli-si}
    S_i = \frac{V[E(Y|X_i)]}{V(Y)} = \frac{y_a \cdot y_c - f_0^2}{y_a \cdot y_a - f_0^2}
\end{equation} where $\cdot$ indicates the dot product and
\begin{equation}\label{eq:f0}
    f_0^2 = \left( \frac{1}{N} \sum_j=1^N y_A^{(j)} \right)^2 \ y_A \cdot y_A .
\end{equation}
The total-effect index can be estimated:
\begin{equation}
    S_{Ti} = 1 - \frac{V[E(Y|X_{\sim i})]}{V(Y)} = 1 - \frac{y_B \cdot y_C - f_0^2}{y_A \cdot y_A - f_0^2}.
\end{equation}
Since the publication of Satelli's approach for computing first-order and total- effects, several algorithmic modifications have been introduced to improve the efficiency and accuracy and the main and total order effects \cite{saltelli2002, MCMC-paper}.

As was the case with MCUQ, the Saltelli approach uses MC to perform sensitivity analysis assuming a non-stochastic solver. While following this workflow with a stochastic solver is possible, the stochastic solver pollutes the results of the sensitivity analysis. With UQ, a fair assumption is that the stochastic solver increases the observed model output variance, possibly causing an analyst to over-estimate the model's response to an uncertain input. In the case of GSA, the effects of the stochastic solver may not be so straightforward; the additional solver variance can also have interaction effects with each of the individual uncertain input parameters. We can see by considering the matrix dimensions of the Saltelli approach that as the dimensionality of the UQ problem increases, over-resolving the stochastic solver to drive the solver variance down is once again extremely computationally expensive. 
%As the dimensionality of the UQ problem increases, over-resolving the stochastic solver, \textit{e.g.} by increasing the number of particle histories, we can see by considering the matrix dimensions of the Saltelli approach how computationally expensive it becomes to use a brute-force method. 
The increased complexity and computational cost provide additional motivation to incorporate a variance deconvolution framework that quantifies and removes stochastic solver variance from Monte Carlo uncertainty quantification and the workflow of global sensitivity analysis.


\section{Background: Monte Carlo radiation transport}\label{sec:background-mcrt}
While a brief discussion of general Monte Carlo sampling methods is included in the background for UQ and GSA, a detailed introduction to Monte Carlo radiation transport methods is also warranted. MC simulations for particle transport treat the physical system of interest as a statistical process, using nuclear data to construct probability distributions that describe the various ways particles can behave in the system. Individual particles are simulated and their behavior (\textit{eg} collisions with other particles, exiting the system, detection at a point in space, etc.) is tallied based on user defined output quantities. The Central Limit Theorem~\cite{Larsen-statistics} can then be applied to extrapolate the tallied behavior of the simulated particle as the average behavior of all particles in the system, with some associated uncertainty based on the number of particles simulated. In contrast,  deterministic transport methods yield an approximation to the transport equation analytically or numerically for average particle behavior across an entire phase space~\cite{mcnp}. 

% why is monte carlo useful
Monte Carlo methods are useful depending on the information needed by the user, the problem space, or the complexity of the equations governing the system. For example, because Monte Carlo methods are event based rather than phase-space based, they can be used to handle time-dependent problems with complicated geometries more effectively than deterministic solvers because there is no need for an accurate discretization scheme, and are useful in cases where a deterministic solution may not be available or accurate \cite{mcnp}. 

% Analog vs. nonanalog definitions
The conceptual baseline of the Monte Carlo method would be to construct the probability distributions that govern the sampled behavior in the simulation directly from physical data, such that the history of each particle follows the exact physics of the problem. This is referred to as analog Monte Carlo, because it is directly analogous to the physical behavior of the particle in a real system~\cite{mcnp}. Even for neutral particles, this direct analog of transport can become restrictively computationally expensive as the physics of the system becomes more complicated. For example, if the tally of interest is geometrically located where few particles end up traveling, it can take a large number of histories to obtain a statistically significant result~\cite{mcnp}. The physics that must be modeled to accurately simulate transport becomes more complicated when considering charged particle transport, and these simulations often incorporate non-analog methods. Non-analog methods, in general, forego the exact physics of a problem in order to reduce computation time or improve scaling with problem size. 

% Why does this matter
As the systems modeled using MCRT become more complex, a single simulation of the model becomes more computationally expensive. In a UQMC or Saltelli method workflow, numerous simulations of the model must be evaluated to compute the UQ and GSA statistics of interest. By incorporating the proposed variance deconvolution method, we aim to make UQ and GSA more tractable for problems with real-world complexity, which would otherwise require very large numbers of particle histories to not just converge, but converge to the point that the MCRT variance can be assumed negligible.

%\textcolor{red}{Include background on CEMeNT challenge problem or more complex/in-depth MCRT physics?}
