
\chapter{Introduction and Background} \label{ch-introduction}
\textcolor{blue}{Last update - 01/09/23}
\section{A brief overview}\label{sec:overview}
As computational modeling becomes more important to the scientific community, so does the necessity of quantifying and analyzing model reliability, accuracy, and robustness. As such, uncertainty quantification (UQ) and global sensitivity analysis (GSA) are important parts of the modeling workflow. Uncertainty quantification has been assigned a number of overlapping definitions - I refer to the mathematical characterization of how sources of input uncertainty affect model output. Global sensitivity analysis can perhaps be viewed as a broadening of uncertainty quantification: GSA aims to apportion, or divide and allocate, uncertainty in model output to different sources of uncertainty in the model input \cite{saltelli2004}. Another way of to phrase this is that GSA aims to understand the relative importance of each of the uncertain inputs, as well as their interactions with one another, to the behavior of model output. As one might imagine, in order to apportion model output uncertainty to various inputs, one must first quantify the uncertainty effects of each of those inputs. In this way, UQ and GSA go hand-in-hand, with UQ typically acting as a step in the GSA workflow. 
There are a wide range of UQ and GSA methodologies which are applicable in different modeling scenarios. In general, though, these methodologies assume that the computational model itself is deterministic, \textit{ie} that given the same input, the model will produce the same output. This way, when analyzing the effects of uncertain input parameters, UQ and GSA methodologies can assume that any output variability is caused by the input variability. If the model incorporates some non-deterministic, \textit{ie} stochastic, behavior, this isn't necessarily the case. A stochastic model that nuclear engineers may be most familiar with is a Monte Carlo radiation transport solver, in which average particle behavior is modeled by sampling probability distributions that describe physical phenomena. In that case, the variance of the model's output is a function (amongst other things) of the number of particle histories run in the simulation. In the case of such a model, the typical UQ workflow which assumes that the output uncertainty can be analyzed as solely the function of uncertain input parameters would be incorrect. This of course propagates through to GSA, absorbing the effects of the solver's stochastic behavior into the output uncertainty, apportioning it to the various UQ inputs, incorrectly analyzing the effects of the uncertain input sources. 

Stochastic solvers are widely used and important for many applications in which physical systems are difficult or outright impossible to describe with deterministic models, and UQ and GSA are too important to forego simply because stochastic solvers make the process less straightforward. A brute-force method to get around this complication is to over-resolve the stochastic solver, e.g. increase the number of particles in the simulation to drive down the variance of the output, rendering it negligible compared to the effects of the uncertain input. Resolving stochastic models to this extent is already computationally expensive, and folding that into the UQ and GSA workflow which requires repeated evaluation of numerical codes increases the computational expense to the point of intractability. As problems become more complex, UQ and GSA become more important, as more and more histories are needed to resolve the solver uncertainty out, and the problem compounds. 

The purpose of this proposal is to quantify and mitigate the effects of using a stochastic solver as part of UQ and GSA workflows. This work will continue ongoing efforts in this space, and the expected outcome is an in-depth discussion of how stochastic solvers can be incorporated into these methods, their performance, and their trade-offs over a collection of realistic scenarios.
While the context of this work is nuclear engineering applications, the work and its results are applicable to a wide range of spaces which use stochastic modeling, such as fluid flow, plasmas, etc.
The remainder of this section provides background on uncertainty quantification, global sensitivity analysis, and Monte Carlo radiation transport. Section~\ref{ch-objectives} is a detailed outline of the research objectives of this proposal. Section~\ref{ch-method} provides the proposed methodology for accomplishing these objectives, as well as supporting theory. I present results for work performed thus far in Section~\ref{ch-results}, with a timeline for remaining work in Section~\ref{ch-schedule}. Finally, Section~\ref{ch-conclusions} contains concluding remarks.

\section{Background: Uncertainty quantification}\label{sec:background-uq}
Scientific progress involves comparing predictions from theory with experimental evidence, with updates to these two pillars happening in tandem, informing one another. Scientists observe and collect data about the physical world, use real-world data to validate or refine theory and models, use those theories and models to predict what other physical phenomena might be observed, compare the models to the data by observing the physical world, etc etc. In \cite{ghanem-uq-handbook}, the authors describe uncertainty quantification as "the rational process by which proximity between predictions and observations is characterized." The process combines applied mathematics, computational science, statistics, and practical knowledge to characterize how well a computational model describes an existing system, or how reliable the model is viewed in predicting new systems.

% Computational models are sometimes described as ``black-boxes,'' which conjure up a useful image for understanding what computational models are. You provide the black-box with some information about a real-world system, it performs some simulation, and outputs some system behavior. 

Computational models are widely useful across any discipline which hopes to take some real-world process -- how the economy might perform under various circumstances, how radiation might move through a system, how disease might move through a population -- and understand it without having to perform real-world experiments. Ideally, a computational model can be built and perfected with existing experimental data, then predict how the system will behave when new conditions are introduced. A model's accuracy can be tested by comparing its prediction to how the system performs in the physical world. This is widely useful, to understand and predict how a system will perform without having to physically construct an experiment, or when this is entirely impossible.

There are a number of types of computational models. One such example is a polynomial whose coefficients are computed using existing data - by plugging the existing data back into the model, one could ensure that they receive the correct known response back. Assuming the model were perfect, they could then plug new inputs into the model how the system would behave. In the nuclear engineering field, we may be more familiar with thinking of computational models as geometric descriptions of systems which incorporate material data to predict how radiation will be transported throughout the system. In this case as well, verification could be performed by comparing with existing benchmarks and experimental data, and once the model has been verified, the system's behavior can be tested under new conditions.

The uncertainty which UQ hopes to quantify can come from a number of sources. Assuming the model is constructed and verified with perfectly accurate data, approximations used to build the model will introduce some discrepancy between the model output and the true real-world behavior. Unfortunately, we can assume that data is not perfectly accurate; even at the limit of perfectly collected data, there will be some unpredictable imperfection which cannot be stamped out. The different types of uncertainty are sometimes broadly categorized as \textit{epistemic} and \textit{aleatoric} \cite{matthies}. Epistemic uncertainty, \textit{ie} systematic uncertainty, arises from the imperfect approximation of system conditions. Approximations may be used for any number of reasons, including assumption that the effects of approximation are negligible, hopes of avoiding computational complexity and expense, or simply the limit of current modeling capabilities. Aleatoric uncertainty, \textit{ie} stochastic uncertainty, describes the impossibility of perfect data. There will be some random imperfections in the machinery, the experiment tools, the conditions of the experiment, etc. which will introduce some uncertainty to the problem. Mathematically, aleatoric uncertainty describes the case in which a system is described by a probability distribution, but what sample would be randomly generated from that distribution is unknown. Epistemic uncertainty describes the case in which the system can not be well-described by a probability distribution, or it's not known how well the distribution describes relevant behavior \cite{smithUQ}. 
% Modern discussion of uncertainty quantification has included questioning whether it is useful or even possible to distinguish between the two types of uncertainty. Any model will use some kind of approximation, and will be built using data that has some aleatoric uncertainty \cite. 
The methods used for UQ depend on the goal of the UQ. In inverse problems, experimental data is used to simultaneously characterize and solve for any unknown model parameters \cite{wildey-presentation}. Examples of inverse problems are when new data are used to update and correct models, or when the solution of the system depends on itself \cite{ghanem-uq-handbook, inverse, pde}. The Bayesian approach is a common UQ method for solving inverse problems\cite{wildey-presentation, ghanem-uq-handbook}. In forward problems, sources of uncertainty are propagated through a model, often with the aim of computing low order moments like mean and variance of the model output \cite{ghanem-uq-handbook}. This is done to evaluate how the system will respond to uncertainty sources and how sensitive the model response is to various sources of uncertainty \cite{saltelliGSA}.

In this work I focus on forward-propagation of aleatoric uncertainty, particularly using Monte Carlo sampling methods. In Monte Carlo UQ (MCUQ), input parameters are randomly sampled from known probability distributions and run through a simulation. This is repeated a number of times such that useful information can be gleaned from the series of outputs: sample moments of the system's average response and variance, a sample-based probability distribution function and cumulative density function of system responses, and the envelope of possible system responses, for example \cite{wildey-presentation}. Unlike polynomial model construction, the cost of MCUQ is independent of problem dimensionality, and converges reliably to a mean system response, albeit slowly, $\mathcal{O}(N^{-1/2}$ for $N$ independent runs \cite{mcnp}. In multi-level and multi-fidelity methods, higher- and lower-fidelity models can be leveraged to allocate the number of samples used and take advantage of this convergence \cite{wildey-presentation}. Workflows for MCUQ are well-established and continue to be optimized for various application spaces \cite{zhang-survey}. However, these methods assume use of a deterministic model, \textit{ie} that given the same input twice, the model will produce the same output. For complex systems, deterministic models are not always well-suited to capture system behavior. One such case is in the use of Monte Carlo radiation transport (MCRT) solvers, wherein the behavior of fixed number of particles is simulated, and the model response is an average of the particle behavior. As with MCUQ, the model output of MCRT solvers converges $\mathcal{O}(N_p^{-1/2}$ for $N_p$ independent particle histories \cite{mcnp}. If traditional MCUQ methods are applied to computational models like this which have some stochastic behavior, the subsequent UQ statistical analysis can be considered ``polluted'' by the variability introduced by the solver itself. When integrating MCRT codes into a UQ workflow, a brute-force method to de-pollute the UQ statistics is to drive down the MCRT solver variability by increasing the number of particle histories \textcolor{red}{[cite paper]}. With this method, the variability contribution from the MCRT solver is assumed to be negligible compared to the variability one hopes to analyze, and the UQ workflow proceeds. However, as mentioned, the variability of the MCRT response is inverse-squarely proportional to the number of particle histories used, likely increasing the computational expense to the point of intractability when combined with the repeated runs necessary for MCUQ. 

Instead, I propose\footnote{I propose? I have developed? I have developed in conjunction with Sandia?} a variance deconvolution framework which quantifies the variance contribution from a stochastic solver, in this case MCRT, and effectively removes it from the total polluted variance, therefore calculating the desired UQ statistics. This is far more cost effective than the brute-force approach, and uses an unbiased estimator for the variance introduced by the solver and for the UQ variance (referred to from here as parametric variance). I apply this variance deconvolution UQ workflow here to MCRT problems, but as I will show, the method is not specific to radiation transport and is widely applicable with solvers that use a Monte Carlo method. 

\section{Background: Global sensitivity analysis}\label{sec:background-gsa}
Global sensitivity analysis is the study of how uncertainty in model output can be apportioned, or divided and allocated, to various sources of uncertainty in the model input \cite{saltelli2004}. Another way of phrasing this is understanding the relative importance of each of the uncertain inputs and their interactions with one another on the behavior of model output. As one might imagine, in order to apportion model output uncertainty to various inputs, one must first quantify the uncertainty effects of each of those inputs. In this way, UQ and GSA go hand-in-hand, with UQ typically acting as a step in the GSA workflow. 

As with uncertainty quantification, global sensitivity analysis is extremely important in understanding model quality and reliability, and should be treated as an important step in the modeling and simulation workflow across all disciplines. There are a number of rationales for incorporating GSA into the computational modeling workflow \cite{saltelli2005}:
\begin{itemize}
    \item Model reliability: Is the model overly-dependent on data which is often very uncertain? Does model uncertainty seem independent of input uncertainty?
    \item Model improvement: Can the model be simplified by simplifying treatment of low-impact parameters? Are there any regions that could benefit from greater study or resolution?
    \item Data prioritization: Factor importance to model output variability correlates with the importance of factor analysis. Would improving the uncertainty of one factor greatly improve output stability? Should the interactions between two inputs be further studied?
\end{itemize}
An additional rationale for GSA which is less-easily bulleted is protection against assumptions which are erroneous but unidentifiable with a finite set of studied parameters. If one only performed MCUQ with a sampled set of input parameters, they could study the effects of those parameters. GSA provides a straightforward and efficient way to know whether the parameters interactions with one another is also of large relative importance. Without this, it could be the case that the specifically sampled input parameters are interacting in a way that is favorable to the output variability, but which is not generally the case, leading to incorrect assumptions about system behavior. 
What is actually studied in GSA? One can compute an importance index for each of the uncertain input parameters \cite{sobol}. Given some generic model $Y=f(X_1,X_2,\cdots,X_k)$ with $k$ uncertain input parameters, consider setting one of the factors $X_i$ equal to some constant $x_i$ within its probability distribution. With this factor frozen, $\Var{Y|X_i=x_i}_{X\sim i}$ would be the variance of output $Y$ dependent on all factors \textit{but} $X_i$. This is known as a conditional variance, because it is conditional on $X_i$ being fixed to $x_i$. To ensure that this conditional variance is not dependent on a particular value of $x_i$, we can compute the average $\Var{Y|X_i=x_i}_{X\sim i}$ over all possible values of $x_i$, $\EE{\Var{Y|X_i=x_i}_{X\sim i}}_{Xi}$. Using the law of total variance \cite{saltelliGSA},
\begin{equation}
    \EE{\Var{Y|X_i=x_i}_{X\sim i}}_{Xi} + \Var{\EE{Y|X_i=x_i}_{X\sim i}}_{Xi} = \Var{Y},
\end{equation}
One can imagine that if $\EE{\Var{Y|X_i=x_i}_{X\sim i}}_{Xi}$ is small, this indicates that all factors which are \textit{not} $X_i$ have a low impact on the output variance, across all possible fixed values of $X_i$; a large $\Var{\EE{Y|X_i=x_i}_{X\sim i}}_{Xi}$ indicates the same. The conditional variance $\Var{\EE{Y|X_i=x_i}_{X\sim i}}_{Xi}$ is also known as the first-order effect of $X_i$ on $Y$. The ratio 
\begin{equation}\label{eq:first-order-si}
    S_i = \frac{\Var{\EE{Y|X_i=x_i}_{X\sim i}}_{Xi}}{V(Y)}
\end{equation}
is ratio of the first-order effect of $X_i$ on $Y$ to the total variance of $Y$. This sensitivity measure, known as the first-order sensitivity index of $X_i$ on $Y$, is numerical measure of the importance of $X_i$ on the variance of output $Y$ \cite{saltelliGSA}.  $S_i$ can range between 0 and 1, where the importance of the variable increases as its $S_i$ approaches 1.
I have also referenced the fact that two input parameters may interact in a way that also affects the output variance. In terms of sensitivity indices, two factors are said to have an interaction effect when their combined effect on $Y$ can not be fully described by the sum of their individual first order effects \cite{saltelliGSA}. The interaction effect of the pair $(X_i,X_j)$ is measured by removing their individual first-order effects from their joint effect,
\begin{equation}\label{eq:interaction-effect}
    V_{ij} = V(f_{ij}(X_i,X_j)) = V(E(Q|X_i,X_j)) - V(E(Y|X_i)) - V(E(Y|X_j)).
\end{equation}
The term $V_{ij}$ is known as the second-order effect, and this methodology can be extended for higher-order effects. From this, we can compute the total effect from parameter $X_i$, which accounts for the individual effect of $X_i$ plus all of its interaction effects. For a model with three uncertain input factors, $Y = f(X_1,X_2,X_3)$, the total effect of $X_1$ is the sum of all terms which include $X_1$,
\begin{equation}\label{eq:total-effect-si}
    S_{T1} = S_1 + S_{12} + S_{13} + S_{123} .
\end{equation}

Though not all GSA methods are variance-based, variance- and variance-decomposition-based techniques are of particular interest for this work. This is not only to take advantage of implementing the variance deconvolution technique introduced in Section~\ref{sec:background-uq}, but also because of their applicability towards some important sensitivity tests. As outlined earlier, there are a number of questions one might hope to answer using these sensitivity indices; analogously, there are a number of sensitivity tests in GSA whose usefulness depends on the problem definition and type of uncertainty. Most relevant to this work are \cite{saltelliGSA}:
\begin{itemize}
    \item Factor Prioritization (FP). Used to identify the input or group of inputs whose variability accounts for most of the output variability. Once identified, focus can be shifted towards reducing the variability of these parameters.
    \item Factor Fixing (FF). Used to identify the input or group of inputs whose variability makes little to no contribution to the output variability. Once identified, these parameters can essentially be set at some arbitrary value within their probability distribution, because varying them does not largely affect the output.
    \item Variance Cutting (VC). Used to identify the smallest set of factors one could act upon in order to reduce the output variance below a given threshold. This ensures the most effective optimization for a given output uncertainty goal.
\end{itemize}

In \cite{saltelliGSA}, Saltelli \textit{et al.} describe what is widely known as the Saltelli method for computing sensitivity indices using Monte-Carlo based sampling. For a model with $k$ uncertain inputs, the following method description is summarized from \cite{saltelliGSA}:
\begin{enumerate}
    \item Generate two $(N,k)$ matrices of random numbers, $A$ and $B$. The base sample $N$ is a number of independent re-samplings of input parameters that can range from a few hundreds to a few thousands. 
    \item Form the matrix $C_i$ by replacing the $i^{th}$ column of $B$ with the $i^{th}$ column of $A$.
    \item Compute the model output as a function of input matrices $A$, $B$, and $C_i$ to obtain $N\times 1$ vectors of model output $y_A=f(A)$, $y_B=f(B)$, and $y_{Ci}=f(C_i)$. 
    \item For all $k$ columns of $A$, construct $C_i$ and compute $y_{Ci}$.
\end{enumerate}
First-order sensitivity indices can be estimated as \cite{saltelliGSA}:
\begin{equation}\label{eq:saltelli-si}
    S_i = \frac{V[E(Y|X_i)]}{V(Y)} = \frac{y_a \cdot y_c - f_0^2}{y_a \cdot y_a - f_0^2}
\end{equation} where $\cdot$ indicates the dot product and
\begin{equation}\label{eq:f0}
    f_0^2 = \left( \frac{1}{N} \sum_j=1^N y_A^{(j)} \right)^2 \ y_A \cdot y_A .
\end{equation}
The total-effect index can be estimated as \cite{saltelliGSA}:
\begin{equation}
    S_{Ti} = 1 - \frac{V[E(Y|X_{\sim i})]}{V(Y)} = 1 - \frac{y_B \cdot y_C - f_0^2}{y_A \cdot y_A - f_0^2}.
\end{equation}
Since the publication of Satelli's approach for computing first-order and total- effects, several algorithmic modifications have been introduced to improve the efficiency and accuracy and the main and total order effects \cite{saltelli2002, MCMC-paper}.

As was the case with the MCUQ workflow, the Saltelli approach uses MC to perform sensitivity analysis assuming a non-stochastic solver. As was the case with MCUQ, while following this workflow with a stochastic solver is possible, the stochastic solver pollutes the results of the sensitivity analysis. With UQ, a fair assumption is that the stochastic solver increases the observed model output variance, possibly causing an analyst to over-estimate the model's response to an uncertain input. In the case of GSA, the interaction among the additional solver variance, each of the individual uncertain input parameters, and the interaction effects of the uncertain input parameters could have different effects on each the sensitivity indices. As the dimensionality of the UQ problem increases, over-resolving the stochastic solver, \textit{eg} by increasing the number of particle histories, we can see by considering the matrix dimensions of the Saltelli approach how computationally expensive it becomes to use a brute-force method. The increased complexity and computational cost provide additional motivation to incorporate a variance deconvolution framework that quantifies and removes stochastic solver variance not just into Monte Carlo uncertainty quantification, but into the workflow of global sensitivity analysis.

\textcolor{red}{Introduce PCE for GSA}

\section{Background: Monte Carlo radiation transport}\label{sec:background-mcrt}
While a brief discussion of general Monte Carlo sampling methods is included in the background for UQ and GSA, it is useful to provide further background on Monte Carlo radiation transport methods. MC simulations for particle transport treat the physical system of interest as a statistical process, using nuclear data to construct probability distributions that describe the various ways particles can behave in the system. Individual particles are simulated and their behavior (\textit{eg} collisions with other particles, exiting the system, detection at a point in space, etc.) is tallied based on what information the user might want. The central limit theorem can then be applied to extrapolate the tallied behavior of the simulated particle as the average behavior of all particles in the system, with some associated uncertainty based on the number of particles simulated. This solution for average particle behavior of interest via extrapolation of tallied particle behavior is as opposed to deterministic transport, which solves an approximation to the transport equation analytically or numerically for average particle behavior across an entire phase space \cite{mcnp}. 

% why is monte carlo useful
Monte Carlo methods are useful depending on the information needed by the user, the problem space, or the complexity of the equations governing the system. For example, because Monte Carlo methods are event based rather than phase-space based, they can be used to handle time-dependent problems with complicated geometries more effectively than deterministic solvers because there is no need for an accurate discretization scheme, and are useful in cases where a deterministic solution may not be available or accurate \cite{mcnp}. 

% Analog vs. nonanalog definitions
The conceptual baseline of the Monte Carlo method would be to construct the probability distributions that govern the sampled behavior in the simulation directly from physical data, such that the history of each particle follows the exact physics of the problem. This is referred to as analog Monte Carlo, because it is directly analogous to the physical behavior of the particle in a real system \cite{mcnp}. Even for neutral particles, this direct analog of transport can become restrictively computationally expensive as the physics of the system becomes more complicated - for example, if the tally of interest is geometrically located where few particles end up traveling, it can take a large number of histories to obtain a statistically significant result \cite{mcnp}. The physics that must be modeled to accurately simulate transport become more complicated when considering charged particle transport, and these simulations often incorporate non-analog methods. Non-analog methods, in general, forego the exact physics of a problem in order to reduce computation time or improve scaling with problem size. 

% Why does this matter
As the systems modeled using MCRT become more complex, a single run of the model becomes more computationally expensive. In a UQMC or Saltelli method workflow, it is unavoidable that this would need to be repeated a number to evaluate the UQ and GSA statistics of interest. By incorporating the proposed variance deconvolution method, I aim to make UQ and GSA more tractable for problems with real-world complexity, which would otherwise require very large numbers of particle histories to not just converge, but converge to the point that the MCRT variance can be assumed negligible.

